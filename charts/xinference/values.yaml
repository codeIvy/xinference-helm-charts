nvidia:
  runtimeClass:
    create: true  # Set to false if RuntimeClass is managed separately

# common used configurations
config:
  xinference_image: "xprobe/xinference:latest-cuda"  # Changed to use CUDA-enabled image
  curl_image: curlimages/curl:8.8.0
  image_pull_policy: "Always"  # Changed to ensure we get the latest image
  worker_num: 1
  gpu_per_worker: "1"  # Explicitly set GPU per worker
  model_src: "huggingface"
  persistence:
    enabled: false
    mountPath: "/data"
  # added NVIDIA and CUDA related environment variables
  extra_envs:
    CUDA_VISIBLE_DEVICES: "0"
    LD_LIBRARY_PATH: "/usr/local/nvidia/lib64:/usr/local/cuda/lib64"
    NVIDIA_DRIVER_VERSION: "535"  # Update this to match your driver version
    TORCH_CUDA_ARCH_LIST: "7.0;7.5;8.0;8.6"  # Adjust based on your GPU architecture

storageClass:
  name: local-storage
  spec:
    provisioner: kubernetes.io/no-provisioner
    volumeBindingMode: WaitForFirstConsumer

pv:
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 20Gi
  hostPath:
    path: /tmp/xinference
  persistentVolumeReclaimPolicy: Delete
  storageClassName: "local-storage"

pvc:
  accessModes:
    - ReadWriteMany
  sharedVolumeClaim:
    storageRequest: 10Gi
  storageClassName: "local-storage"
  volumeMode: "Filesystem"

serviceWeb:
  ports:
  - nodePort: 30003
    port: 9997
    protocol: TCP
    targetPort: 9997
  type: NodePort

serviceSupervisor:
  ports:
  - name: service-supervisor-oscar
    port: 9999
    protocol: TCP
    targetPort: 9999
  - name: service-supervisor-web
    port: 9997
    protocol: TCP
    targetPort: 9997
  type: ClusterIP

serviceWorker:
  ports:
  - port: 30001
    protocol: TCP
    targetPort: 30001
  type: ClusterIP

xinferenceSupervisor:
  supervisor:
    args:
    - --port
    - "9997"
    - --host
    - $(POD_IP)
    - --supervisor-port
    - "9999"
    - --log-level
    - debug
    ports:
      - containerPort: 9997
        name: web
      - containerPort: 9999
        name: oscar
    resources:
      requests:
        cpu: "1"
        memory: 4Gi
        nvidia.com/gpu: "1"  # Added GPU request
      limits:
        cpu: "2"
        memory: 8Gi
        nvidia.com/gpu: "1"  # Added GPU limit

xinferenceWorker:
  strategy:
    type: Recreate
  worker:
    initContainers:
      command: [ 'sh', '-c', "until curl -v http://service-supervisor:9997/v1/address; do echo waiting for supervisor; sleep 1; done" ]
    args:
    - -e
    - http://service-supervisor:9997
    - --host
    - $(POD_IP)
    - --worker-port
    - "30001"
    - --log-level
    - debug
    ports:
      - containerPort: 30001
    resources:
      requests:
        cpu: "2"
        memory: 8Gi
        nvidia.com/gpu: "1"  # Added GPU request
      limits:
        cpu: "4"
        memory: 16Gi
        nvidia.com/gpu: "1"  # Added GPU limit
